{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOe5EseVUIY5NPzrzU4nDly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelCondo/Team9-APS360-Project/blob/main/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4_z0JJHB7Fd"
      },
      "source": [
        "# Check Yo' Self Demo\n",
        "**Self-serve checkout enhancement using Neural Networks and Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hntxRytfB-Xh"
      },
      "source": [
        "## Architecture\n",
        "![alt text](https://github.com/MichaelCondo/Team9-APS360-Project/blob/main/docs/Architecture_Diagram.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOyhZVLvBxzU"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Seed for reproducible results\n",
        "np.random.seed(1000)\n",
        "torch.manual_seed(1000)\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt07jCbFCd0g"
      },
      "source": [
        "## Load Dataset & Mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JANekU0ACEjL"
      },
      "source": [
        "# Download the dataset from GitHub\n",
        "!wget -nc https://github.com/Horea94/Fruit-Images-Dataset/archive/master.zip \\\n",
        "    && mkdir -p \"/root/project\" \\\n",
        "    && unzip -nq \"master.zip\" -d \"/root/project/datasets\" \\\n",
        "    && find \"/root/project/datasets/\" -mindepth 2 -maxdepth 2 -type d -ls\n",
        "DATA_MASTER_PATH = \"/root/project/datasets/Fruit-Images-Dataset-master\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whHDgiQCs3L"
      },
      "source": [
        "import requests\n",
        "\n",
        "CLASS_MAPPING = requests.get(\"https://raw.githubusercontent.com/MichaelCondo/Team9-APS360-Project/main/mappings/original_to_new_class.json\").json()\n",
        "CLASS_PRICES = requests.get(\"https://raw.githubusercontent.com/MichaelCondo/Team9-APS360-Project/main/mappings/class_to_price.json\").json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuavMXdpChG0"
      },
      "source": [
        "## Count Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1elvp7cQCmK9"
      },
      "source": [
        "!echo \"Training samples\" && find \"{DATA_MASTER_PATH}/Training\" -type f | wc -l \n",
        "!echo \"Testing samples\" && find \"{DATA_MASTER_PATH}/Test\" -type f | wc -l \n",
        "!echo \"Multiple Fruits Testing samples\" && find \"{DATA_MASTER_PATH}/test-multiple_fruits\" -type f | wc -l \n",
        "\n",
        "!echo \"Training samples class distribution\" \\\n",
        "    && cd \"{DATA_MASTER_PATH}/Training\" \\\n",
        "    && find -maxdepth 2 -mindepth 2 -type f -printf \"%h\\0\" \\\n",
        "    | uniq -zc | tr '\\0' '\\n' | sort -rh  \\\n",
        "    | (sed -u 10q ; echo \"...\" ; tail)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywOLOminCnG5"
      },
      "source": [
        "## PreProcess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cBhudVkDA5u"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "for dataset_type in [\"Training\", \"Test\", \"test-multiple_fruits\"]:\n",
        "    for f in os.scandir(f\"{DATA_MASTER_PATH}/{dataset_type}\"):\n",
        "        if f.is_dir() and f.name not in CLASS_MAPPING:\n",
        "            shutil.rmtree(f.path)\n",
        "            print(\"Removed\", dataset_type, \"/\", f.name)\n",
        "\n",
        "!cd \"{DATA_MASTER_PATH}/Training\" \\\n",
        "    && find -maxdepth 2 -mindepth 2 -type f -printf \"%h\\0\" \\\n",
        "    | uniq -zc | tr '\\0' '\\n' | sort -rh  \\\n",
        "    | (sed -u 10q ; echo \"...\" ; tail)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycIdDmxDX_5"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAYJqJdcEce1"
      },
      "source": [
        "class Augment(object):\n",
        "    \"\"\"Augment image by modifying background colour to something random.\"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        img = pic.convert(\"RGB\")\n",
        "        datas = img.getdata()\n",
        "        \n",
        "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
        "\n",
        "        newData = []\n",
        "        for item in datas:\n",
        "            if item[0] >= 240 and item[1] >= 240 and item[2] >= 240:\n",
        "                newData.append(color)\n",
        "            else:\n",
        "                newData.append(item)\n",
        "\n",
        "        img.putdata(newData)\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOpkQSV7Da3i"
      },
      "source": [
        "class Brightness(object):\n",
        "    \"\"\"Augment image by modifying background colour to something random.\"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        return transforms.functional.adjust_brightness(pic, brightness_factor=1.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr4rWRoKD4sj"
      },
      "source": [
        "class Contrast(object):\n",
        "    \"\"\"Augment image by modifying background colour to something random.\"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        return transforms.functional.adjust_contrast(pic, contrast_factor=1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03MKOsTD4-Y"
      },
      "source": [
        "class RemappedImageFolder(torchvision.datasets.ImageFolder):\n",
        "    \"\"\"Subclass of ImageFolder that lets you remap your classes at parse time\n",
        "    instead of through target_transform (which would be applied on every getitem)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_remapping, **kwargs):\n",
        "        self.class_remapping = class_remapping\n",
        "        super(RemappedImageFolder, self).__init__(**kwargs)\n",
        "\n",
        "    def _find_classes(self, dir):\n",
        "        \"\"\"Override ImageFolder's class identification method so we can\n",
        "        do custom consolidation of our classes.\n",
        "        See https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n",
        "        \"\"\"\n",
        "        # Get the original classes (i.e. folder names)\n",
        "        o_classes = super()._find_classes(dir)[0]\n",
        "\n",
        "        # Get our classes (from the JSON mapping)\n",
        "        classes = list(sorted(set(self.class_remapping.values())))\n",
        "\n",
        "        # Create an index for only our new classes\n",
        "        new_class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "\n",
        "        # Map the old classes to their indices in our new system\n",
        "        real_class_to_idx = {\n",
        "            old_class: new_class_to_idx[self.class_remapping[old_class]]\n",
        "            for old_class in o_classes\n",
        "        }\n",
        "\n",
        "        return classes, real_class_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOqMAnIbD5Po"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_data_loaders(batch_size=1, num_workers=1):\n",
        "    \"\"\"Loads images of produce, splits the data into training, validation\n",
        "    and testing datasets. Returns data loaders for the three preprocessed datasets.\n",
        "\n",
        "    Args:\n",
        "        batch_size: A int representing the number of samples per batch (<1 means \n",
        "        use the full dataset in every batch)\n",
        "    \n",
        "    Returns:\n",
        "        train_loader: iterable training dataset (from training data)\n",
        "        val_loader: iterable validation dataset (from training data)\n",
        "        test_loader: iterable testing dataset (from test data)\n",
        "        classes: A list of strings denoting the name of each class\n",
        "    \"\"\"\n",
        "    ########################################################################\n",
        "    # We'll return a (train, val, test, classes) tuple\n",
        "    classes = list(sorted(set(CLASS_MAPPING.values())))\n",
        "\n",
        "    # Load datasets\n",
        "    train_set = RemappedImageFolder(class_remapping=CLASS_MAPPING,\n",
        "            root=f\"{DATA_MASTER_PATH}/Training\",\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.RandomApply([transforms.RandomCrop(80), Brightness(), Contrast()], p=0.80),\n",
        "                 transforms.Resize((100, 100)),\n",
        "                 transforms.ToTensor()]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    test_set = RemappedImageFolder(class_remapping=CLASS_MAPPING,\n",
        "            root=f\"{DATA_MASTER_PATH}/Test\",\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.Resize((100, 100)), \n",
        "                 transforms.ToTensor()]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    # Split train/validation data with stratification of classes\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        np.arange(len(train_set.targets)),\n",
        "        test_size=0.3,\n",
        "        shuffle=True,\n",
        "        stratify=train_set.targets\n",
        "    )\n",
        "\n",
        "    train_set, val_set = [\n",
        "        torch.utils.data.Subset(train_set, idx)\n",
        "        for idx in [train_idx, val_idx]\n",
        "    ]\n",
        "\n",
        "    # Create and return the data loaders\n",
        "    bs = len(train_set) if batch_size < 1 else batch_size\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=bs, num_workers=num_workers, shuffle=True\n",
        "    )\n",
        "    \n",
        "    bs = len(val_set) if batch_size < 1 else batch_size\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_set, batch_size=bs, num_workers=num_workers, shuffle=True\n",
        "    )\n",
        "    \n",
        "    bs = len(test_set) if batch_size < 1 else batch_size\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_set, batch_size=bs, num_workers=num_workers, shuffle=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dEDI1upFKVd"
      },
      "source": [
        "# Count the samples\n",
        "for kind, loader in zip([\"Training\", \"Validation\", \"Test\"], get_data_loaders()[:3]):\n",
        "    print(f\"Found {len(loader)} {kind} examples\")\n",
        "\n",
        "# Prepare dataloaders for real\n",
        "train_loader, val_loader, test_loader, classes = get_data_loaders(\n",
        "    batch_size=27  # we know that this is a divisor of the number of submissions\n",
        ")\n",
        "\n",
        "print(\"There are\", len(classes), \"classes\")\n",
        "\n",
        "print(\"Training set looks like:\")\n",
        "# Obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()  # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1TEcGtxGbem"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld6RIpvdIINp"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple CNN classifier that takes a list of output classes on init\n",
        "    to determine the output layer size. Assumes an input of size nx3x100x100.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        num_outputs = len(output_classes)\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 2, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, 2, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 250),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(250, num_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCzcYLDpIjZb"
      },
      "source": [
        "model = CNN(classes)\n",
        "# See if dimensions are okay\n",
        "model(torch.randn((64, 3, 100, 100)))\n",
        "\n",
        "# See model stats\n",
        "print(model.__class__.__name__, \"parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70CPDSD2Itz-"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6MoKT1RK7OR"
      },
      "source": [
        "def get_model_name(model, batch_size=1, learning_rate=1e-3, iteration=0):\n",
        "    \"\"\"Generate a name for the model with the hyperparameter values\"\"\"\n",
        "    return \"model_{0}_bs{1}_lr{2}_checkpoint{3}\".format(\n",
        "        model.__class__.__name__, batch_size, learning_rate, iteration\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, criterion=None):\n",
        "    \"\"\"Evaluate the model (accuracy, loss) on the given dataset\n",
        "    If critierion is not provided, only accuracy will be computed.\n",
        "    \"\"\"\n",
        "    correct, total = 0, 0\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in iter(data_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Calculate batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Correctness: select index with maximum prediction score\n",
        "        pred = outputs.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += pred.shape[0]\n",
        "\n",
        "        # Optional loss computation\n",
        "        if criterion:\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    if criterion:\n",
        "        return correct / total, total_loss / len(data_loader)\n",
        "    else:\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "def plot_training_curve(path):\n",
        "    \"\"\" Plots accuracy/loss curves for a model run given the CSV path\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    iterations, train_acc, val_acc, train_loss, val_loss = np.loadtxt(\n",
        "        f\"{model_path}_training.csv\"\n",
        "    ).T\n",
        "\n",
        "    plt.title(\"Train vs Validation Accuracy\")\n",
        "    plt.plot(iterations, train_acc, label=\"Train\")\n",
        "    plt.plot(iterations, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(iterations, train_loss, label=\"Train\")\n",
        "    plt.plot(iterations, val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        \"Best Training Accuracy: {:.2f}% @ checkpoint {:.0f}\".format(\n",
        "            max(train_acc) * 100, iterations[np.argmax(train_acc)]\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"Best Validation Accuracy: {:.2f}% @ checkpoint {:.0f}\".format(\n",
        "            max(val_acc) * 100, iterations[np.argmax(val_acc)]\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shb8RbXFK96j"
      },
      "source": [
        "def train(model, batch_size=1, num_epochs=1, learning_rate=1e-3, plot_frequency=1):\n",
        "    train_loader, val_loader, test_loader, classes = get_data_loaders(\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Statistic collection\n",
        "    PLOT_ITERATIONS = int(-(-len(train_loader) // plot_frequency))  # ceil division\n",
        "    iterations, train_acc, val_acc, train_loss, val_loss = [], [], [], [], []\n",
        "\n",
        "    def collect_stats(epoch, iteration, t_loss=None):\n",
        "        # Don't double count\n",
        "        if iterations and iterations[-1] >= iteration:\n",
        "            return\n",
        "        # Compute and store\n",
        "        if t_loss is None:\n",
        "            t_acc, t_loss = evaluate(model, train_loader, criterion)\n",
        "        else:\n",
        "            t_acc = evaluate(model, train_loader)\n",
        "\n",
        "        v_acc, v_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        iterations.append(iteration)\n",
        "        train_acc.append(t_acc)\n",
        "        val_acc.append(v_acc)\n",
        "        train_loss.append(t_loss)\n",
        "        val_loss.append(v_loss)\n",
        "\n",
        "        # Checkpoint model\n",
        "        model_path = get_model_name(model, batch_size, learning_rate, iteration)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        # Print stats\n",
        "        print(\n",
        "            f\"@{time.time()-start_time : .2f}s\",\n",
        "            f\"Epoch {epoch} Iteration {iteration}:\",\n",
        "            f\"Train acc: {t_acc * 100 : .2f}%\",\n",
        "            f\"Train loss: {t_loss : .4f}\",\n",
        "            \"|\",\n",
        "            f\"Validation acc: {v_acc * 100 : .2f}%\",\n",
        "            f\"Validation loss: {v_loss : .4f}\",\n",
        "        )\n",
        "\n",
        "    # Training loop\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the network\n",
        "    print(\"Training Started...\")\n",
        "    start_time = time.time()\n",
        "    iteration = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in iter(train_loader):\n",
        "            # Place on correct device if available\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Forward pass, backward pass, optimize, and clean up\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute training metrics\n",
        "            iteration += 1\n",
        "            if iteration % PLOT_ITERATIONS == 0:\n",
        "                collect_stats(epoch, iteration, loss.item())\n",
        "\n",
        "    # Final stats (runs only if previous iteration wasn't saved)\n",
        "    collect_stats(epoch, iteration, loss.item())\n",
        "\n",
        "    print(f\"Finished Training -- Total time elapsed: {time.time()-start_time : .2f}s\")\n",
        "    model_path = get_model_name(model, batch_size, learning_rate, iteration)\n",
        "    np.savetxt(\n",
        "        f\"{model_path}_training.csv\",\n",
        "        np.c_[iterations, train_acc, val_acc, train_loss, val_loss],\n",
        "    )\n",
        "    return model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk8hdmFXPzeL"
      },
      "source": [
        "model = CNN(classes)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "model_path = train(\n",
        "    model,\n",
        "    batch_size=27,\n",
        "    num_epochs=15,\n",
        "    learning_rate=1e-3,\n",
        "    plot_frequency=1,\n",
        ")\n",
        "print(\"Model available at:\", model_path)\n",
        "plot_training_curve(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGtKkRbnP1kk"
      },
      "source": [
        "model = CNN(classes)\n",
        "\n",
        "model.load_state_dict(torch.load(get_model_name(model, 27, 1e-3, 19208)))\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "test_acc = evaluate(model, get_data_loaders(-1)[2])\n",
        "print(\"Testing Accuracy: {:.2f}%\".format(test_acc * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_CEsxChP2OR"
      },
      "source": [
        "print(\"Test set verification:\")\n",
        "\n",
        "# Obtain one batch of test images\n",
        "dataiter = iter(get_data_loaders(-1, 0)[2])\n",
        "images, labels = dataiter.next()\n",
        "# print(type(images))\n",
        "# print(images.shape)\n",
        "# for image in images:\n",
        "#   print(type(image))\n",
        "#   break\n",
        "out = model(images.cuda())\n",
        "pred = out.max(1, keepdim=True)[1]\n",
        "images = images.numpy()  # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "    ax.set_title(classes[pred[idx]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKGXTVu7Epdd"
      },
      "source": [
        "## Object Localization and Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwInnSm0IDlV"
      },
      "source": [
        "import torchvision\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from matplotlib import cm\n",
        "!pip install ffmpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvkA58DIIFbD"
      },
      "source": [
        "model_restNet = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model_restNet.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKMqG6vpILL0"
      },
      "source": [
        "# Give an image and get the bounding boxes \n",
        "def get_prediction(img_path, threshold, open=False):\n",
        "  if open:\n",
        "    img = Image.open(img_path) # Load the image\n",
        "  else:\n",
        "    img = img_path\n",
        "  transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform\n",
        "  img = transform(img) # Apply the transform to the image\n",
        "  pred = model_restNet([img]) # Pass the image to the model\n",
        "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())] # Bounding boxes\n",
        "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "  pred_t = [pred_score.index(x) for x in pred_score if x > threshold] # Get list of index with score greater than threshold.\n",
        "\n",
        "  if len(pred_t) > 0:\n",
        "    pred_t = pred_t[-1]\n",
        "    pred_boxes = pred_boxes[:pred_t+1]\n",
        "    return pred_boxes\n",
        "  else:\n",
        "    pred_t = 0\n",
        "    return []\n",
        "  \n",
        "\n",
        "# Crop each image with the bounding boxes \n",
        "def seg_img (img_path, boxes, plot=False, open=False):\n",
        "  img_list = []\n",
        "  for i in range(len(boxes)):\n",
        "    if open:\n",
        "      im = Image.open(img_path) \n",
        "    else:\n",
        "      im = img_path\n",
        "      im = Image.fromarray(np.uint8(im)).convert('RGB')  #convert numpy array to PIL image\n",
        "    im = im.crop((boxes[i][0][0], boxes[i][0][1], boxes[i][1][0], boxes[i][1][1]))\n",
        "    img_list.append(im)\n",
        "    if plot:\n",
        "      plt.imshow(im)\n",
        "      plt.show()\n",
        "  return img_list\n",
        "\n",
        "# ref: https://stackoverflow.com/questions/44231209/resize-rectangular-image-to-square-keeping-ratio-and-fill-background-with-black/44231784\n",
        "def make_square(im, min_size=100, fill_color=(0, 0, 0, 0)):\n",
        "    x, y = im.size\n",
        "    size = max(min_size, x, y)\n",
        "    new_im = Image.new('RGB', (size, size), fill_color)\n",
        "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
        "    return new_im\n",
        "\n",
        "def classify_cropped_images(seg_img_list, plot=False):\n",
        "  images = []\n",
        "  for im in seg_img_list:\n",
        "    im = make_square(im, fill_color=(255, 255, 255, 0))\n",
        "    im = im.resize((100, 100)) # #Resize Image to 100, 100\n",
        "    im = ToTensor()(im) # #convert PIL image to tensor\n",
        "    images.append(im)\n",
        "\n",
        "  images = torch.stack(images)\n",
        "  model = CNN(classes)\n",
        "  model.load_state_dict(torch.load(get_model_name(model, 27, 1e-3, 13720)))\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "  out = model(images.cuda())\n",
        "  pred = out.max(1, keepdim=True)[1]\n",
        "  images = images.numpy()               # convert images to numpy for display\n",
        "\n",
        "  # plot the images in the batch, along with the corresponding labels\n",
        "  if plot:\n",
        "    fig = plt.figure(figsize=(25, 4))\n",
        "    for idx in np.arange(len(pred)):\n",
        "        ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
        "        plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "        ax.set_title(classes[pred[idx]])\n",
        "  \n",
        "  predictionClasses = []\n",
        "  for p in pred:\n",
        "    predictionClasses.append(classes[p])\n",
        "  return predictionClasses\n",
        "\n",
        "def object_detection_api(img_path, open=False, plot=False, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
        "  boxes = get_prediction(img_path, threshold, open) # Get predictions\n",
        "  pred_cls = []\n",
        "\n",
        "  if len(boxes) > 0:\n",
        "    seg_img_list = seg_img(img_path, boxes, plot, open)\n",
        "    pred_cls = classify_cropped_images(seg_img_list, plot)\n",
        "\n",
        "  new_boxes = []\n",
        "  for box in boxes:\n",
        "    new_box = [box[0][0], box[0][1], box[1][0], box[1][1]]\n",
        "    new_boxes.append(new_box)\n",
        "\n",
        "  return new_boxes, pred_cls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWxt4lpsIL9H"
      },
      "source": [
        "%cd '/content/drive/MyDrive/APS360 Project/model_checkpoint'\n",
        "boxes, pred_cls = object_detection_api('/root/project/datasets/Fruit-Images-Dataset-master/test-multiple_fruits/apple_pear.jpg', open=True, plot=False, threshold=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVEOWi78JWsZ"
      },
      "source": [
        "### Receipt Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SIUqrjBJYKZ"
      },
      "source": [
        "def create_receipt(pred_cls):\n",
        "  \"\"\"\n",
        "    cust_cart = {'apple': ['count', 'price']}\n",
        "  \"\"\"\n",
        "  cust_cart = {}\n",
        "  for pred in pred_cls:\n",
        "    if pred in cust_cart:\n",
        "      cust_cart[pred][0] += 1\n",
        "    else:\n",
        "      cust_cart[pred] = []\n",
        "      cust_cart[pred].append(1) \n",
        "      cust_cart[pred].append(CLASS_PRICES[pred]) \n",
        "  return cust_cart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9flEjZ0JaI4"
      },
      "source": [
        "cart = create_receipt(pred_cls)\n",
        "subtotal = 0\n",
        "for fruit, info in cart.items():\n",
        "  price = round(info[0] * info[1], 2)\n",
        "  print(fruit + \": \" + str(info[0]))\n",
        "  print(\"\\t\" + str(info[0]) + \" x \" + str(price) + \"\\t$\" + str(price))\n",
        "  subtotal += price\n",
        "\n",
        "subtotal = round(subtotal, 2)\n",
        "print(\"\\nSubtotal: $\" + str(subtotal))\n",
        "\n",
        "tax = 1.13\n",
        "total = round(subtotal * tax, 2)\n",
        "print(\"Total: $\" + str(total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4zjo2P-JhJq"
      },
      "source": [
        "## Detecting Objects in Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkUrtPkcEsxK"
      },
      "source": [
        "classes = list(sorted(set(CLASS_MAPPING.values())))\n",
        "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrdIyRkWEsge"
      },
      "source": [
        "def draw_boxes(boxes, preds, image):\n",
        "    # read the image with OpenCV\n",
        "    for i, box in enumerate(boxes):\n",
        "        pred_idx = classes.index(preds[i])\n",
        "        color = COLORS[pred_idx]\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])),\n",
        "            (int(box[2]), int(box[3])),\n",
        "            color, 4\n",
        "        )\n",
        "        cv2.putText(image, preds[i], (int(box[0]), int(box[1]-5)),\n",
        "                    cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), 1, \n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhxWX2tSEzL5"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "## remove previous files to avoid overlap\n",
        "%cd '/content/drive/MyDrive/APS360 Project/output'\n",
        "!rm -r output.avi\n",
        "!rm -r output.mp4\n",
        "!ls \n",
        "\n",
        "# reset directory to model checkpoints\n",
        "%cd '/content/drive/MyDrive/APS360 Project/model_checkpoint'\n",
        "\n",
        "# All the test video\n",
        "test_vids_paths = [ '/content/drive/MyDrive/APS360 Project/fruit_video_Trim.mp4',\n",
        "                    '/content/drive/MyDrive/APS360 Project/Fruit_Vid_001.mp4'          \n",
        "                  ]\n",
        "\n",
        "# Set path for your test vid\n",
        "vid_path = test_vids_paths[1]\n",
        "\n",
        "# Create a VideoCapture object\n",
        "cap = cv2.VideoCapture(vid_path)\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if (cap.isOpened() == False): \n",
        "  print(\"Unable to read camera feed\")\n",
        "\n",
        "# Create VideoWriter object and write to output file \"output.avi\"\n",
        "out_dim_x = 1280\n",
        "out_dim_y = 720\n",
        "out = cv2.VideoWriter('/content/drive/MyDrive/APS360 Project/output/output.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (out_dim_x, out_dim_y))\n",
        "\n",
        "# Number of frames in the video\n",
        "print(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "\n",
        "while(cap.isOpened()):\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  if ret == True: \n",
        "    with torch.no_grad():\n",
        "        frame = cv2.resize(frame, (out_dim_x, out_dim_y))\n",
        "        boxes, preds = object_detection_api(frame)\n",
        "        \n",
        "    # Draw boxes around dtected objects in current frame\n",
        "    if len(boxes) > 0:\n",
        "      image = draw_boxes(boxes, preds, frame)\n",
        "      out.write(image)  # Write the frame into the file 'output.avi'\n",
        "\n",
        "    # Press Q on keyboard to stop recording\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "      break\n",
        "\n",
        "  # Break the loop\n",
        "  else:\n",
        "    break  \n",
        "\n",
        "# When everything done, release the video capture and video write objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cppUoY8UJq_I"
      },
      "source": [
        "import ffmpy\n",
        "\n",
        "ff = ffmpy.FFmpeg(\n",
        "    inputs={'/content/drive/MyDrive/APS360 Project/output/output.avi':None},\n",
        "    outputs={'/content/drive/MyDrive/APS360 Project/output/output.mp4': None}\n",
        ")\n",
        "\n",
        "ff.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC2YXXKVJv9F"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/APS360 Project/output/output.mp4\"\n",
        "\n",
        "mp4 = open(video_path, \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=800 controls autoplay loop>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}